kubectl config view    >> Status of k8 cluster
minikube start
minikube status
minikube stop

Create first deployment 
kubectl create deployment hello-nodes --image=k8s.gcr.io/echoserver:1.4
kubectl get deployment
kubectl get pods

Expose the deployment to create service which can be accessed thru external world
kubectl expose deployment hello-nodes --type=LoadBalancer --port=8080
kubectl get services
minikube service hello-nodes

Cleanup
kubectl delete service hello-nodes
kubectl delete deployment hello-nodes


Namespace : are the ways to separate and organize objects in K8s
kubectl get namespaces
kubectl get pods --namespace <namespace_name>
kubectl get pods --all-namespaces
kubectl create namespace <namespace_name>

High Availability in K8s cluster >> Multiple control plane >> Multi master and multi worker node toplogoy
User will need LB to communciate with multiple control planes
+
Worler node will also communcate with LB

ETCD Management : 

Stacked ETCD :
2 mater node >> 2 control palne >>  scheduler , controler, etcd, api server
two etcd will communicate with each other called stacked etcd

External ETCD : Remove etcd from each control plane
Each etcd is maintained on separate server, two etcd will communicate with each other 


K8s Management Tools :
kubectl 
kubeadm : Create the K8 cluster and helping users to make the cluster functional
Minikube
Helm : Poweful tool for K8s tempate and package mgt, Ability to convert K8s objetcs in reusable templates, Ability to provide complex multi 
configation template with cross connectivity

Kompose : translate docker compose file into kubernetes object
Kustomize : Configuration  Mgt tool similar to HELM




************* HA K8s Cluster *************
************* Install Kubernertes on Master Node *************

1. Upgrade apt packages
sudo apt-get update

2. Install Docker Engine
sudo apt-get install -y docker.io

3. Install Support packages
sudo apt-get install -y apt-transport-https curl

4. Retrieve the key for the Kubernetes repo and add it to your key manager
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -

5. Add the kubernetes repo to your system
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF

6. Install the three pieces you’ll need, kubeadm, kubelet, and kubectl
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

7. Create the actual cluster
kubeadm init --pod-network-cidr=192.168.0.0/16

Note - Notedown the Command to join cluster

8. Install the Calico network plugin
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml

9. Untaint the master so that it will be available for scheduling workloads
kubectl taint nodes --all node-role.kubernetes.io/master-

10. Get Cluster Nodes
kubectl get nodes


*****************************************************************************

************* Install & Configure Kubernertes Worker Node *************

1. Upgrade apt packages
sudo apt-get update

2. Install Docker Engine
sudo apt-get install -y docker.io

3. Install Support packages
sudo apt-get install -y apt-transport-https curl

4. Retrieve the key for the Kubernetes repo and add it to your key manager
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -

5. Add the kubernetes repo to your system
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF

6. Install the three pieces you’ll need, kubeadm, kubelet, and kubectl
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

7. Join the Worker Nodes to the Cluster
In the Control Plane Node, create the token and copy the kubeadm join command (NOTE:The join command can also be found in the output from kubeadm init command):
kubeadm token create --print-join-command

8. In both Worker Nodes, paste the kubeadm join command to join the cluster:
sudo kubeadm join <join command from previous command>

9. In the Control Plane Node, view cluster status (Note: You may have to wait a few moments to allow the cluster to become ready):
kubectl get nodes

**********************************************************************************************************************************************************






Node Draining in K8s HA Cluster. >> Worked on Node Draining inside kubernetes cluster
----------------------------------

kubectl get pods -o wide    >> which pod is running on which node

Remove the worker node from cluster , to apply security patches, add some additional disc, Increase the capacity of worker node
Application should have no downtime

Containers running on node should be gracefully terminated and should be rescheduled on avilable worker node

kubectl drain <nodename> >> make sure no deamonset running on node
deamonset >> pods that are tightly coupled to worker node, like monitoring , health check, performance check that are running on each worker node
kubectl drain <nodename> --ignore-daemnonsets



kubectl get nodes
kubectl get pods -o wide
kubectl apply -f myfile.yml >> Create Sample Pods and deployments
kubectl drain <nodename> --ignore-daemonsets --force
kubectl get nodes
kubectl get pods -o wide (Pods got rescheduled)

Rejoin >> 
kubectl uncordon <nodename>
kubectl get nodes
kubectl get pods -o wide


Upgrading the K8s Cluster. >> Worked on K8s cluster update periodic task to keep the cluster sync with latest K8s release using kubeadm
--------------------------

Master Node Upgrade

kubectl get nodes
kubectl drain <master-node-name> --ignore-daemonsets

Upgrade Kubeadm
sudo apt-get install -y --allow-change-held-packages kubeadm=1.21.1-00
kubeadm version

Verify the upgaraded plan
sudo kubeadm upgrade plan v1.21.1-00

Apply the upgraded plan
kubeadm upgrade apply v1.21.1-00


Upgrade kubelet and kubectl packages
sudo apt-get update
sudo apt-get install -y --allow-change-held-packages kubelet=1.21.1-00 kubectl=1.21.1-00

Restart the kubelet
sudo systemctl daemon-reload
sudo systemctl restart kubelet

kubectl get nodes
kubectl uncordon <master-node>



Worker Node Upgrade >> Rolling upgrade, Blue Green Upgrade , No app down time

kubectl drain <worker-node-name> --ignore-daemonsets --force
sudo apt-get update

Upgrade Kubeadm
sudo apt-get install -y --allow-change-held-packages kubeadm=1.21.1-00
kubeadm version

Upgrade kubelet config
sudo kubeadm upgrade node


Upgrade kubelet and kubectl packages
sudo apt-get update
sudo apt-get install -y --allow-change-held-packages kubelet=1.21.1-00 kubectl=1.21.1-00

Restart the kubelet
sudo systemctl daemon-reload
sudo systemctl restart kubelet

kubectl get nodes
kubectl uncordon <master-node>
***************************************************************************************************************************************************


Application Config : K8s allows users to pass the dynamic conf values to app at runtime line app.properties file
These dynamic conf help users to control the app flow


2 ways to supply  dynamic conf values to app:
Config Maps : Non Sensitive Data , Stores th data in key value format, prevents hardcoding config data to pod specifications
Secrets : Sensitive Data




***********************************************************************************************************

Limit Resouce Usage on Container and Nodes


Request: The number of resources being requested for a container. If a container exceeds its request for resources, it can be throttled back down to its request.

Limit: An upper cap on the resources a single container can use. If it tries to exceed this predefined limit it can be terminated if K8's decides that another 
container needs these resources. If you are sensitive towards pod restarts, it makes sense to have the sum of all container resource limits equal to or less 
than the total resource capacity for your cluster.


apiVersion: v1
kind: Pod
metadata:
 name: demo
spec:
 containers:
 - name: example1
 image:example/example1
 resources:
   requests:
     memory: "_Mi"
     cpu: "_m"
   limits:
     memory: "_Mi"
     cpu: "_m"

***********************************************************************************************************
Worked on monitoring the container resoureces in K8s cluster with Liveness, Readiness and StartUp probes

Liveness Probe : continious check the container healtchek. helps to determine the container state.
Restart the failing containers but that also depends upon the restart policy of your application
Active Healthcheck. If liveness probe is failing, it will auto restart your containers
User can execute 2 types of liveness probe : 
   1. Running commands in containers
   2. Periodic HTTP Healthcheck

initialDelaySeconds : How long to wait before sending a probe after container starts
periodSeconds : How often a probe will be sent
timeOutSeconds : How long a request can take to respond before it's considered as failure


StartUp Probe :
Long Startup time - Legagy kind of application
Liveness probe will try to start the containers and it may go in infinite loop forever
StartUp Probe runs at a container startup and stop running at container success.
As soon as first startup probe is success, it will exit out and it will never execute on your container.
It will not send the liveness probe until first startup probe will be success.

failureThreshold: 30 >> Number of failures in startup probe
periodSeconds: 10

Applciation will have 30*10 = 5 min to finish its startup


Readiness Probe : My app is ready to consume the traffic
Front end app/containes started quickly + DB app backend app/containers taking some time to respond
Readiness probe will  app  is completely ready to serve the traffic
NO Tranfiic is sent to containers untill the first readiness probe will be success

load large DS, Configuration file during the startup, external DB


**********************************************************************************************************
Container restart Policies :

 Always       : Even if container exited successfully : Is a default
 OnFailure    : On Failure
 Never        : Never
 
**********************************************************************************************************
Multiple Containers in Single Pod :
Share the same network, storage and communicate on localhost
Best Practice : keep the containers in separate pods, until we would like the containers to share the resource


Network : Conatiners can communciate on any port unless the port is exposed to cluster
Storage : Shared volume to share data 

apiVersion: v1
kind: Pod
metadata:
  name: two-containers
spec:
  restartPolicy: OnFailure
  containers:
    - name: nginx-container
      image: nginx
      volumeMounts:
        - name: shared-data
          mountPath: /usr/share/nginx/html
    - name: debian-container
      image: debian
      volumeMounts:
        - name: shared-data
          mountPath: /pod-data
      command: ["/bin/sh"]
      args: ["-c", "echo Hello from the Secondary container > /pod-data/index.html"]
  volumes:
    - name: shared-data
      emptyDir: {}
     
***********************************************************************************************************************************



Scheduler >> Responsible to schedule pods on nodes and that scheduling depends upon multiple factors :

Schedules select the nodes on the basis of: 
1. Resource Req Vs Avaible Node Resources
   4 core cpu , 2 GB Resource req >> Only one worker node is capable to handle that request, schedule on tht node
   
   
apiVersion: v1
kind: Pod
metadata:
  name: frontend-app
spec:
  containers:
    - name: app
      image: alpine
      command:
        - sleep
        - '3600'
      resources:
        requests:
          memory: 64Mi
          cpu: 1000m
  nodeSelector:
    disktype: ssd
   
2. Configuration like node labels
   Define the lables on node and use the same labels on pod/container specification
   K8 read the manifest file and match the labels
   
3. node Selector, affinity and anti affinity
   node Selector use the labels (key value pair) to select the suitable node 
   
   kubectl get node --show-labels
   kubectl label nodes <node_name> disktype=ssd
   
   
apiVersion: v1
kind: Pod
metadata:
  name: nginx-nodeselector
spec:
  containers:
    - name: nginx
      image: nginx
  nodeSelector:
    disktype: ssd
    
4. node name
   manifest file we can define the node name
   
apiVersion: v1
kind: Pod
metadata:
  name: nginx-nodename
spec:
  containers:
    - name: nginx
      image: nginx
  nodeName: k8s-worder-01

***************************************************************************************************************************************


Daemon Set : Runs copy of pod on each node
Daemon Set runs copy of pod on new node as they are added to the cluster
Daemon Set will be helpful in case of Monitoring, Log Collection and proxy configuration


apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: logging
spec:
  selector:
    matchLabels:
      app: httpd-logging           >> Selector for Daemon set must match with POD Label
  template:
    metadata:
      labels:
        app: httpd-logging          >> Label Attached to the pod
    spec:
      containers:
        - name: webserver
          image: httpd
          ports:
            - containerPort: 80
            
 ********************************************************************************************************************************************
 
 Static Pod in K8s, Mirror Pods
 
 Static Pod are directly managed by kubelet on K8s nodes
 They do not requuire any control plane or API Server
 Kubelet watches each static pod and restarts if its fail
 
 Monitoring, Configuration without Control Server/ API Server
 
 On any node go to /etc/kubernetes/manifests/ >> Create Yml file
 to strt the pod >> sudo systemctl restart kubelet
 
 It will create a mirror pod on master node and try to delete the pod and it will not get deleted because User can't change or update static pods via mirror pods
 
 Mirror Pod : created for each static pod, monitor the static pod via K8s APIs
 User can't change or update static pods via mirror pods
 
 **************************************************************************************************************************************************
 
 Node Affinity and Anti Affinity
 
 Node Affinity is enhanced version of Node Selector, used for pod allocation on worker node
 Not to schedule pod on node is achieved by Anti Affinity, opp to affinity and node selector
 
apiVersion: v1
kind: Pod
metadata:
  name: nginx-nodeaffinity
spec:
  containers:
    - name: nginx
      image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: disktype
                operator: In
                values:
                  - ssd
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx-node-anti-affinity
spec:
  containers:
    - name: nginx
      image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: disktype
                operator: NotIn
                values:
                  - ssd
 
 
 
