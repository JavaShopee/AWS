Worked on managing container resources in K8s cluster

Installation Steps for Minikube :

1. Install Docker CE Edition    > 20.10.6
2. Install Kubectl              > 1.21.1            
3. Install Minikube             > 1.20.0

K8S Version > 1.20.2

kubectl config view    >> Status of k8 cluster
minikube start
minikube status
minikube stop

Create first 

kubectl create deployment hello-nodes --image=k8s.gcr.io/echoserver:1.4
kubectl get deployment
kubectl get pods

Expose the deployment to create service which can be accessed thru external world
kubectl expose deployment hello-nodes --type=LoadBalancer --port=8080
kubectl get services
minikube service hello-nodes

Cleanup
kubectl delete service hello-nodes
kubectl delete deployment hello-nodes


Namespace : are the ways to separate and organize objects in K8s
kubectl get namespaces
kubectl get pods --namespace <namespace_name>
kubectl get pods --all-namespaces
kubectl create namespace <namespace_name>

High Availability in K8s cluster >> Multiple control plane >> Multi master and multi worker node toplogoy
User will need LB to communciate with multiple control planes
+
Worler node will also communcate with LB

ETCD Management : 

Stacked ETCD :
2 mater node >> 2 control palne >>  scheduler , controler, etcd, api server
two etcd will communicate with each other called stacked etcd
NOTE : Kubeadm installation followsthe stacked ETCD Model

External ETCD : Remove etcd from each control plane
Each etcd is maintained on separate server, two etcd will communicate with each other 


K8s Management Tools :
kubectl 
kubeadm : Create the K8 cluster and helping users to make the cluster functional
Minikube
Helm : Poweful tool for K8s tempate and package mgt, Ability to convert K8s objetcs in reusable templates, Ability to provide complex multi 
configation template with cross connectivity

Kompose : translate docker compose file into kubernetes object
Kustomize : Configuration  Mgt tool similar to HELM




************* HA K8s Cluster *************
************* Install Kubernertes on Master Node *************

1. Upgrade apt packages
sudo apt-get update

2. Install Docker Engine
sudo apt-get install -y docker.io

3. Install Support packages
sudo apt-get install -y apt-transport-https curl

4. Retrieve the key for the Kubernetes repo and add it to your key manager
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -

5. Add the kubernetes repo to your system
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF

6. Install the three pieces you’ll need, kubeadm, kubelet, and kubectl
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

7. Create the actual cluster
kubeadm init --pod-network-cidr=192.168.0.0/16

Note - Notedown the Command to join cluster

8. Install the Calico network plugin
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml

9. Untaint the master so that it will be available for scheduling workloads
kubectl taint nodes --all node-role.kubernetes.io/master-

10. Get Cluster Nodes
kubectl get nodes


*****************************************************************************

************* Install & Configure Kubernertes Worker Node *************

1. Upgrade apt packages
sudo apt-get update

2. Install Docker Engine
sudo apt-get install -y docker.io

3. Install Support packages
sudo apt-get install -y apt-transport-https curl

4. Retrieve the key for the Kubernetes repo and add it to your key manager
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -

5. Add the kubernetes repo to your system
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF

6. Install the three pieces you’ll need, kubeadm, kubelet, and kubectl
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

7. Join the Worker Nodes to the Cluster
In the Control Plane Node, create the token and copy the kubeadm join command (NOTE:The join command can also be found in the output from kubeadm init command):
kubeadm token create --print-join-command

8. In both Worker Nodes, paste the kubeadm join command to join the cluster:
sudo kubeadm join <join command from previous command>

9. In the Control Plane Node, view cluster status (Note: You may have to wait a few moments to allow the cluster to become ready):
kubectl get nodes

**********************************************************************************************************************************************************






Node Draining in K8s HA Cluster. >> Worked on Node Draining inside kubernetes cluster
----------------------------------

kubectl get pods -o wide    >> which pod is running on which node

Remove the worker node from cluster , to apply security patches, add some additional disc, Increase the capacity of worker node
Application should have no downtime

Containers running on node should be gracefully terminated and should be rescheduled on avilable worker node. Not applicaple for Bare pos.

kubectl drain <nodename> >> make sure no deamonset running on node
deamonset >> pods that are tightly coupled to worker node, like monitoring , health check, performance check that are running on each worker node
kubectl drain <nodename> --ignore-daemnonsets



kubectl get nodes
kubectl get pods -o wide
kubectl apply -f myfile.yml >> Create Sample Pods and deployments
kubectl drain <nodename> --ignore-daemonsets --force
kubectl get nodes
kubectl get pods -o wide (Pods got rescheduled)

Rejoin >> 
kubectl uncordon <nodename>
kubectl get nodes
kubectl get pods -o wide

Bare Podswill NOT Join


Upgrading the K8s Cluster. >> Worked on K8s cluster update periodic task to keep the cluster sync with latest K8s release using kubeadm
--------------------------

Master Node Upgrade

kubectl get nodes
kubectl drain <master-node-name> --ignore-daemonsets

Upgrade Kubeadm
sudo apt-get install -y --allow-change-held-packages kubeadm=1.21.1-00
kubeadm version

Verify the upgaraded plan
sudo kubeadm upgrade plan v1.21.1-00

Apply the upgraded plan
kubeadm upgrade apply v1.21.1-00


Upgrade kubelet and kubectl packages
sudo apt-get update
sudo apt-get install -y --allow-change-held-packages kubelet=1.21.1-00 kubectl=1.21.1-00

Restart the kubelet
sudo systemctl daemon-reload
sudo systemctl restart kubelet

kubectl get nodes
kubectl uncordon <master-node>



Worker Node Upgrade >> Rolling upgrade, Blue Green Upgrade , No app down time

kubectl drain <worker-node-name> --ignore-daemonsets --force
sudo apt-get update

Upgrade Kubeadm
sudo apt-get install -y --allow-change-held-packages kubeadm=1.21.1-00
kubeadm version

Upgrade kubelet config
sudo kubeadm upgrade node


Upgrade kubelet and kubectl packages
sudo apt-get update
sudo apt-get install -y --allow-change-held-packages kubelet=1.21.1-00 kubectl=1.21.1-00

Restart the kubelet
sudo systemctl daemon-reload
sudo systemctl restart kubelet

kubectl get nodes
kubectl uncordon <master-node>
***************************************************************************************************************************************************


kubectl create vs kubectl apply : If the resource already exists kubectl create will give you an error

kubectl describe pod <pod_name>
kubectl get pods -o wide
kubectl get pods -o json
kubectl get pods -o yaml
kubectl exec <pod name> -c <container_name> -- cat /etc/nginx/nginx.conf




***************************************************************************************************************************************************

Role Based Access Mgt (RBAC)   > Can't provide admin access to all the individuals working in org
Allows manage the user access in the K8S cluster
Example : Limited access to Deployments and Pods

K8S RBAC Objects > Roles and ClusterRoles > Define set of permission
Role > Define permission within namespace, limited to Namespace
ClusterRoles > Define permission within cluster, not limited to NS, limited to cluster

RoleBinding binds the role to the user
ClusterRolesBinding : Binds the clsuter roles to the user 


Steps >> Add User in K8S cluster
      >> Create Role
      >> Create Role Biding






***************************************************************************************************************************************************

Application Config : K8s allows users to pass the dynamic conf values to app at runtime line app.properties file
These dynamic conf help users to control the app flow


2 ways to supply  dynamic conf values to app:
Config Maps : Non Sensitive Data , Stores th data in key value format, prevents hardcoding config data to pod specifications
Secrets : Sensitive Data



We can pass config Map and Secrets to Containrs at runtime :
Using > Env Vars, Mount Vols, posix

In posix we dont need to define separate env var for each key value pair 
there is one to one mapping b/w key value pair unlike using env vars




***********************************************************************************************************

Limit Resouce Usage on Container and Nodes


Request: The number of resources being requested for a container. If a container exceeds its request for resources, it can be throttled back down to its request.

Limit: An upper cap on the resources a single container can use. If it tries to exceed this predefined limit it can be terminated if K8's decides that another 
container needs these resources. If you are sensitive towards pod restarts, it makes sense to have the sum of all container resource limits equal to or less 
than the total resource capacity for your cluster.
If CPU is excedded, throttle the process
If memory is excedded, terminate the containers


apiVersion: v1
kind: Pod
metadata:
 name: demo
spec:
 containers:
 - name: example1
 image:example/example1
 resources:
   requests:
     memory: "_Mi"
     cpu: "_m"
   limits:
     memory: "_Mi"
     cpu: "_m"

***********************************************************************************************************
Worked on monitoring the container resoureces in K8s cluster with Liveness, Readiness and StartUp probes

Liveness Probe : continious check the container healtchek. helps to determine the container state.
Restart the failing containers but that also depends upon the restart policy of your application
Active Healthcheck. If liveness probe is failing, it will auto restart your containers
User can execute 2 types of liveness probe : 
   1. Running commands in containers
   2. Periodic HTTP Healthcheck

initialDelaySeconds : How long to wait before sending a probe after container starts
periodSeconds : How often a probe will be sent
timeOutSeconds : How long a request can take to respond before it's considered as failure


StartUp Probe :
Long Startup time - Legagy kind of application
Liveness probe will try to start the containers and it may go in infinite loop forever
StartUp Probe runs at a container startup and stop running at container success.
As soon as first startup probe is success, it will exit out and it will never execute on your container.
It will not send the liveness probe until first startup probe will be success.

failureThreshold: 30 >> Number of failures in startup probe
periodSeconds: 10

Applciation will have 30*10 = 5 min to finish its startup


Readiness Probe : My app is ready to consume the traffic
Front end app/containes started quickly + DB app backend app/containers taking some time to respond
Readiness probe will  app  is completely ready to serve the traffic
NO Tranfiic is sent to containers untill the first readiness probe will be success

load large DS, Configuration file during the startup, external DB


**********************************************************************************************************
Container restart Policies :

 Always       : Even if container exited successfully : Is a default
 OnFailure    : On Failure
 Never        : Never
 
**********************************************************************************************************
Multiple Containers in Single Pod :
Share the same network, storage and communicate on localhost
Best Practice : keep the containers in separate pods, until we would like the containers to share the resource


Network : Conatiners can communciate on any port unless the port is exposed to cluster
Storage : Shared volume to share data 

apiVersion: v1
kind: Pod
metadata:
  name: two-containers
spec:
  restartPolicy: OnFailure
  containers:
    - name: nginx-container
      image: nginx
      volumeMounts:
        - name: shared-data
          mountPath: /usr/share/nginx/html
    - name: debian-container
      image: debian
      volumeMounts:
        - name: shared-data
          mountPath: /pod-data
      command: ["/bin/sh"]
      args: ["-c", "echo Hello from the Secondary container > /pod-data/index.html"]
  volumes:
    - name: shared-data
      emptyDir: {}
     
***********************************************************************************************************************************

INIT Containers : Specialized containers that runs before the app containers in POD
Utility / Setup Script that are NOT the part of app containers
App Image : Less Bulky and light weight

Will execute in sequence

***********************************************************************************************************************************
Scheduler >> Responsible to schedule pods on nodes and that scheduling depends upon multiple factors :

Schedules select the nodes on the basis of: 
1. Resource Req Vs Avaible Node Resources
   4 core cpu , 2 GB Resource req >> Only one worker node is capable to handle that request, schedule on tht node
   
   
apiVersion: v1
kind: Pod
metadata:
  name: frontend-app
spec:
  containers:
    - name: app
      image: alpine
      command:
        - sleep
        - '3600'
      resources:
        requests:
          memory: 64Mi
          cpu: 1000m
  nodeSelector:
    disktype: ssd
   
2. Configuration like node labels
   Define the lables on node and use the same labels on pod/container specification
   K8 read the manifest file and match the labels
   
3. node Selector, affinity and anti affinity
   node Selector use the labels (key value pair) to select the suitable node 
   
   kubectl get node --show-labels
   kubectl label nodes <node_name> disktype=ssd
   
   
apiVersion: v1
kind: Pod
metadata:
  name: nginx-nodeselector
spec:
  containers:
    - name: nginx
      image: nginx
  nodeSelector:
    disktype: ssd
    
4. node name
   manifest file we can define the node name
   
apiVersion: v1
kind: Pod
metadata:
  name: nginx-nodename
spec:
  containers:
    - name: nginx
      image: nginx
  nodeName: k8s-worder-01

***************************************************************************************************************************************


Daemon Set : Runs copy of pod on each node
Daemon Set runs copy of pod on new node as they are added to the cluster
Daemon Set will be helpful in case of Monitoring, Log Collection and proxy configuration
Daemon Set follow the normal scheduling rules around the node labels, taints and tolerations



apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: logging
spec:
  selector:
    matchLabels:
      app: httpd-logging           >> Selector for Daemon set must match with POD Label
  template:
    metadata:
      labels:
        app: httpd-logging          >> Label Attached to the pod
    spec:
      containers:
        - name: webserver
          image: httpd
          ports:
            - containerPort: 80
            
 ********************************************************************************************************************************************
 
 Static Pod in K8s, Mirror Pods
 
 Static Pod are directly managed by kubelet on K8s nodes
 They do not requuire any control plane or API Server
 Kubelet watches each static pod and restarts if its fail
 
 Use Cases : Whenever you want to perform some bootstrap on your nodes, go for statci pod
 
 Monitoring, Configuration without Control Server/ API Server
 
 On any node go to /etc/kubernetes/manifests/ >> Create Yml file
 to strt the pod >> sudo systemctl restart kubelet
 
 It will create a mirror pod on master node and try to delete the pod and it will not get deleted because User can't change or update static pods via mirror pods
 
 Mirror Pod : created for each static pod, monitor the static pod via K8s APIs
 User can't change or update static pods via mirror pods
 
 **************************************************************************************************************************************************
 
 Node Affinity and Anti Affinity
 
 Node Affinity is enhanced version of Node Selector, used for pod allocation on worker node
 Not to schedule pod on node is achieved by Anti Affinity, opp to affinity and node selector
 
 Node Affinity have enhanced syntax as compared to Node Selector
 requiredDuringSchedulingIgnoredDuringExecution  : Hard Affinity
 prefferedDuringSchedulingIgnoredDuringExecution  : Soft Affinity
 
apiVersion: v1
kind: Pod
metadata:
  name: nginx-nodeaffinity
spec:
  containers:
    - name: nginx
      image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: disktype
                operator: In
                values:
                  - ssd
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx-node-anti-affinity
spec:
  containers:
    - name: nginx
      image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: disktype
                operator: NotIn
                values:
                  - ssd
 
 
 *******************************************************************************************************************************************************
 Stateless App
 Application data geenrated in one session can not be used in next session with that client
 can be scaled horizontally
 New Pods can be created for Stateless App
 
 
 Stateful App
 Data that is saved from the activities of one session for the use in next session
 DB Services are stateful services, they have the file system that can't be split in multiple instances
 can be scaled vertically
 
 
 
 Replication Controller : Desired number of pods are up and running at spefied point of time.
 apiVersion: v1
kind: ReplicationController
metadata:
  name: alipne-box-replicationcontroller
spec:
  replicas: 3
  selector:
    app: alipne-box
  template:
    metadata:
      name: alpine
      labels:
        app: alipne-box
    spec:
      containers:
      - name: alpine-box
        image: alpine
        command: ["sleep", "3600"]
        
kubectl scale --replicas <name_of_RC>




*******************************************************************************************************
Replica Set

Enhanced Version of replication controller
Purpose is stable set of pod replicase are up and running  at given point of time


Diff B/w Replica Set and Replication Controller : Selector Support
Replication Controller : Selector Set is a key value pair and must match with the labels of pods

Replica Set  : Selector set is match expression where we can provide the conditional stmts ( In , NotIn, Exists) 	
Allows to use “set based” label selector


apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-replicas
  labels:
    app: myapp
    tier: frontend
spec:
  replicas: 3
  selector:
    matchExpressions:
      - {key: tier, operator: In, values: [frontend]}
  template:
    metadata:
      labels:
        app: myapp
        tier: frontend
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80



*****************************************************************

Deployment in K8s

Deployment is one step higher than Replica Set, control both replica set and pod 

Use case : Create Deployment, Update deployment, Rolling Upgrade, Rollback, Pause/Resume Deployment

Kubectl rollout status <deployment_name>
Kubectl get rs

apiVersion: apps/v1
kind: Deployment
metadata:
  name: chef-server
  labels:
    app: chef
spec:
  replicas: 3
  selector:
    matchLabels:
      app: chef-server
  template:
    metadata:
      labels:
        app: chef-server
    spec:
      containers:
        - name: chef-server
          image: 'chef/chefdk:4.9'
          ports:
            - containerPort: 8080
          command:
            - /bin/sh
          args:
            - '-c'
            - echo Hello from the Chef container; sleep 3600
        - name: ubuntu
          image: 'ubuntu:18.04'
          ports:
            - containerPort: 8080
          command:
            - /bin/sh
          args:
            - '-c'
            - echo Hello from the Ubantu container; sleep 3600
            
            
 kubectl set image <delpoyment name> <container name>=<image name>
 kubectl set image delpoyment/chef-server chef-server=chef/chefdk:4.9.10 --record
 kubectl rollout status depolyment/chef-server
 kubectl rollout history depolyment/chef-server
 
 kubectl rollout undo depolyment/chef-server
 kubectl rollout undo depolyment/chef-server --to-revision=1
 
 kubectl edit depolyment/chef-server


kubectl rollout pause depolyment/chef-server >> Bulk Changes
kubectl rollout resume depolyment/chef-server
kubectl scale depolyment/chef-server --replicas=5


*********************************************************************************************************************************************

Service is used to access the application running on pods

Replication Controller : Pods are dynmaic in nature, created and terminated during scaling
Deployment : Image Version Upgrade, pods are dynamically created and terminated
Pods can not be accessed directly but thru a service

Publish the service : service types
-----------------------------------

Cluster IP : Publish the service on cluster internal IP so tht other pods can access the service if they want
Choosing this value makes the service only reachable from within the cluster



NodePort : Exposes the service at each node id at static port.
You will be able to. connect the nodeport service from outside the cluster by requesting NodeId:Nodeport

Load Balancer : Expose the service externally using cloud provider load balancer
load balancer will route the traffic to ClusterIP and NodePort


minikube service <service-name> --url
 
 
 *****************************************************************************************************************************
 
 key value pair that are attached to objects
 can be attached to objetcs at the time of creation and sebsequently modified and added at any time
 Once lables are attahced to the objects we can filter the result on the basis of labels >> Label Selectors
 multiple lable with single objs ans single label with mutliple objs
 
 Once nodes are tagged, we can use label selector to run podes on specific nodes
 
 kubectl label nodes <node-name> disktype=ssd
 kubectl get nodes --show-labels
 Inside the pod :


apiVersion: v1
kind: Pod
metadata:
  name: nginx-web
  labels:
    app: nginx-pod
spec:
  containers:
  - name: nginx
    image: nginx:1.16.0
    ports:
      - name: nginxport
        containerPort: 80
  nodeSelector:
    disktype: ssd
  
  
 *******************************************************************************************************************************************
 
 
 Pod Lifecycle
 
 
 Pending : Pod is accepted by kubernetes but container is not yet started
 Running : at least one container is running
 Succedded : All Succeded 
 Failed : at least one failed
 CrashLoopBackOff : trying to start again and again


Also Go to Udemy

apiVersion: v1
kind: Pod
metadata:
  name: lifecycle-demo
spec:
  containers:
  - name: lifecycle-demo-container
    image: nginx
    lifecycle:
      postStart:
        exec:
          command: ["/bin/sh", "-c", "echo Hello from the postStart
handler > /usr/share/message"]
      preStop:
        exec:
          command: ["/bin/sh","-c","nginx -s quit; while killall -0
nginx; do sleep 1; done"]


****************************************************************************************************************************

DNS allows communication between the pods
kube-dns runs as a service in kube-system namespace

Pod on node can communicate with all pods on all nodes without NAT
Agents (system daemons, kubelet) can communicate with pod on that node
Every pod >> unique IP addr

All pods in Kubeadm cluster will be given the domain name like :
pod-ip.namespace-name.pod.cluster.local
pods indside kubernertes clustr comminicate via IP addr or dns

************************************************************************************************************************************************************************************

CNI Plugins : Nodes will remain NOTREADY until you install Network Plugin and the user won't be able to run pods
CNI Plugins provides the connectivty b/w podss as per K8s n/w model. Multiple CNI N/W plugins are available like Calico Network plugins




************************************************************************************************************************************************************************************

CNI Plugins : Nodes will remain NOTREADY until you install Network Plugin and the user won't be able to run pods
CNI Plugins provides the connectivty b/w podss as per K8s n/w model. Multiple CNI N/W plugins are available like Calico Network plugins



******************************************************************************************
Worked on network policies in K8S cluster to control the traffic flow at IP addr level and port level
Network policies are used to control traffic flow at IP address and port level

Purpose : Keeping the pods isolated from traffic they don't need

Pods can communicate using 3 identifiers :
Other pods that are allowed
Namespaces that are allowed
IP Blocks

By default, Pods are Non Isolated, accept the traffic from all sources


N/W Policy Components :
Pod Selector : to which pod in the Namespace n/w policy will be applied
               pod selectort can select the pods using matching labels
               empty pod selector select all pods in the namespace
               
               N/w policy apply on both Ingress(traffic coming to pod from other source) and Egress traffic(traffic leaving the pod to dest)
               
               fromSelector : Ingress Traffic
               toSelector : Egress traffic
 Namespace selector : Select namespace to allow traffic from/to
 IP Block : Selct IP Range to allow traffic from/to
 Port : Specify one or more port that allow traffic



apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: sample-network-policy
  namespace: network-policy
spec:
  podSelector:
    matchLabels:
      app: frontend
  policyTypes:
  - Ingress
  - Egress
  ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            role: test-network-policy
      ports:
      - protocol: TCP
        port: 80


******************************************************************************************************************************************

K8S Services : Access the pods to outside world
Abstract layer b/w pods and client
Way to expose app as a set of pods


Service Routing : Client make the request to service which routes the traffic to pods in Load Balancer fashion

End Point : Backend entity tht will connect service and pods
Combinattion of pod ip and pod port

*******************************************************************************************************************************************

Service Type : how and where service will expose the app
Defaut is cluster ip

Cluter IP, NodePort, LB, Ext Name

Cluter IP : Expose the service within the cluster network, client is within the cluster
Use Cluster IP when client is other pods within the cluster belonging to diff namespace

By default comm b/w pods in enaled thru IP asdd but what when pods belongs to diff ns ? >> Cluster IP

NodePort : expose application outside cluster network, client is asscessing the service from outside the network.

LB: Expose service to outer world but cloud ILB is required.
You are using the K8s services on some public cloud and that public cloud supports some kind ok LB. like AWS, Azure, GCP
Backend LB will connect to NodePort and NodePort will comnnect to POD

Extenal Name  : LB Service type you need to creaate the CNAME which will directly access the service in K8


**********************************************************************************************************************************


Service DNS Names : allow application within the cluster to easily locate the service
Service-name.namespace-name.svc.cluster-domain.example >> Use if the pods and service are in diff NS
Service-name.namespace-name.svc.cluster.local



****************************************************************

Ingress Controller : is used to maange the external access to service in K8S cluster

External Client ----------> Ingress Controller ---------------> Services


Ingress Controller : is the obj b/w client and services, abstarct interface for providing more secure traffic
Apart from Node port service, SSL Termination, LB, NameBase Virtual Hosting

Ingress defines set of routing rules
Each rule has a set of path each with the backend. Request matching a path will be routed to associated backend


Ingress >> Corresponding Hosts >> Service >> Dep >> Pod > Cont

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-rules
spec:
  rules:
  - host: nginx-official.example.com
    http:
      paths:
      - path: /
        pathType: Exact
        backend:
          service:
            name: nginx-official-service
            port:
              number: 80
  - host: magical-nginx.example.com
    http:
      paths:
      - path: /
        pathType: Exact
        backend:
          service:
            name: magical-nginx
            port:
              number: 80



****************************************************************

Data Storage In K8s


Container File system is ephemeral. Files in the container FS exists as lonk as container exists.
Data in the container FS is long as soon as the container is deleted or recreated.

The problem is there in Stateful apps. Solutions is volumes.

Volumes allow to send store the data outside containers while allowing containers to access the data at runtime.
Real time replication : What ever changes are there in container FS, same changes will also be replicatied to External File System.

Persitent Volumes are bit more adv than volumes. Persitent Volumes allows users to treat storage as an abstract resource and consume it using pods.


Volume Types : NFS, Cloud Storage, Config Maps and Secrets, File Sys on K8s node

********************************************************************************************************************************

Volume and Volume Mount
Volume : is the POD specifcation, User can define storage vol available in for the pod
Volume Mount is the container spec, refers the vol in pod spec and provides the mount path


apiVersion: v1
kind: Pod
metadata:
  name: hostpath-pod
spec:
  volumes:
  - name: hostpath-vol
    hostPath:
      path: /var/tmp.                    >> Location on host machine/node
  containers:
  - name: hostpath-pod
    image: 'k8s.gcr.io/busybox'
    command: ["/bin/sh", "-c", "echo Hello Team, This is Sample File for HostVolume - $(date) >> /output/output.txt"]
    volumeMounts:
    - name: hostpath-vol
      mountPath: /output                  >> within the container
      
      
      
apiVersion: v1
kind: Pod
metadata:
  name: redis-emptydir
spec:
  containers:
  - name: redis
    image: redis
    volumeMounts:
    - name: redis-storage
      mountPath: /data/redis
  volumes:
  - name: redis-storage
    emptyDir: {}                          >> Dynamic loc within the host machine
    
    
 emptyDir data persist until the pod persists. If POD is deleted, data will get deleted.
 If container is killed (without killling the POD), data will still persist.
 
Conatier can be killed by killing the process.
emptyDir : Mutiple contaiers within the pod can refer the same emptyDir vol




******************************************************************************************************************************************************

Shared Vol by NGINX and Debian Containers

apiVersion: v1
kind: Pod
metadata:
  name: shared-multi-container
spec:
  volumes:
    - name: html
      emptyDir: {}
  containers:
    - name: nginx-container
      image: nginx
      volumeMounts:
        - name: html
          mountPath: /usr/share/nginx/html
    - name: debian-container
      image: debian
      volumeMounts:
        - name: html
          mountPath: /html
      command:
        - /bin/sh
        - '-c'
      args:
        - while true; do date >> /html/index.html; sleep 5; done
        


****************************************************************************************************************************************


Persistent Volume in K8S
Persistent Volume is K8S obj which allows user to treat storage as an abstract resource to pods
Persistent Volume is the resource in the cluster just like node

with Persistent Volume, you can define the storage as local disk or cloud storage

Storage Class : Another K8s obj, Specify all type of storage service they offer on their platform
                Somebody needs to control where that where the user is creating the persitent vol, that control is given by storage class

within the storage class, allowVolumeExpansion : Ability to resize storage class once they are created. true /false


persistentVolumeReclaimPolicy:
Reclaim Policy : How the storage class will be used when the PVs assocaited with PVCs will be deleted

                 Retain : Keep All data 
                 Delete : Delete underlying storage resorces
                 Recycle : Delete underlying storage data and allow PVs to be reuse
                 
PVC is the request for storage by the user. Another K8S obj. PVC looks for PV that is able to meet the criteria, found, then bind PV to PVC
without PVC, you can not attch p vols to your resouces

***********************************************************************************************************************

Troubleshoot K8S Cluster


If API Server is down, 
Connection to the server localhost:6443 was refused. Did you specify the rigth port and host?

Make sure docker and kubelet services are up and running on master node


Node Status :

kubectl get nodes
kubectl describe node <node name>

sudo systemctl status docker
sudo systemctl status kubelet
sudo systemctl start kubelet
sudo systemctl enable kubelet.  >> If the node is going to reboot


All pods in kube system name sapace shoule be in the running state
kubectl get pods -n kube-system
kubectl describe pod pod-name -n kube-system


**********************************************************************************************************


Logs of K8S services on each node :

sudo journalctl -u kubelet -n 100 (Tail last 100 lines)
sudo journalctl -u kubelet -f (follow the log)
sudo journalctl -u docker


Default log output redirected to :
/var/log/kube-apiserver.log
/var/log/kube-scheduler.log
/var/log/kube-control-manager.log


not for kubeadm >> Executing everything as a pod

kubectl logs -n kube-system <pod name>


************************************************************************************************************************

Troubleshoot Applications in K8S


kubectl describe pod pod-name
kubectl exec pod pod-name -c container-name -- command

*********************************************************************************************************************
Worked on troubleshooting the self managed K8S cluster
Container Log
kubectl logs pod-name -c container-name


****************************************************************

Worked on Deploying and Upgrading Sping Boot pplication in K8S cluster using Helm Charts

Packaging and deploying the app in K8S with the utility : HELM

HELM is K8S based package installer

Chart Structure >> Chart.yml , template, Values.yml

which helm

helm create <chart name>
helm create springboot

tree springboot

helm template <chart name>  >> fill all the placeholders
heml lint <chart name>    >> detects any problems, syntax error, compilation


helm install myfirstspringboot springboot
helm install <release name> <chart name>

helm list -a
kubectl get all



kubectl get all >> Verify K8 Cluster

Upgrade Helm Release
______________________

update Chart.yml with version num
update Values.yml with replica num

helm upgrade <release name> <chart name>
helm upgrade myfirstspringboot springboot
helm rollback myfirstspringboot 1
helm rollback <release name> <revision num>

helm list -a
kubectl get all

helm delete myfirstspringboot
helm list -a



Handle Up Stream repo in helm chrt
__________________________________
helm repo list
helm repo add <repo name> <repo url>
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo list
helm search repo bitnami

helm repo update >> Pull all latest changes in local env
helm repo index helloworld. >> generates index.yml in helm chart

helm repo remove bitnami



